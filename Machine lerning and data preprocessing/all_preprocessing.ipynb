{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1802aa3-1a80-4c9f-82a1-1aaa60161d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import json \n",
    "from math import cos, sin, atan2, sqrt, radians, degrees\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44a36c75-336e-4699-9072-c11a9619bf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "ELASTIC_USERNAME = \"team11\"\n",
    "ELASTIC_PASSWORD = \"v6Smlb2yINWc4\" # INSERT YOUR PASSWORD\n",
    "\n",
    "hosts = [\"https://eestec2024.cl-test.medius.si:9200\"]\n",
    "# How many samples the query returns (maximum is 10000) if you want more use scroll\n",
    "number_of_samples = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897560f8-bde3-439b-bdf9-9934f95e7133",
   "metadata": {},
   "source": [
    "# initialize client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4db73244-f163-457e-8dca-aba199f243f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the Elasticsearch cluster\n",
    "es = Elasticsearch(\n",
    "    hosts,\n",
    "    ssl_show_warn=False,  # Suppress SSL warnings, use with caution\n",
    "    verify_certs=False,  # This skips SSL certificate verification\n",
    "    basic_auth=(ELASTIC_USERNAME, ELASTIC_PASSWORD)  # For basic authentication\n",
    "    # bearer_auth='Bearer <token>'  # For bearer token authentication, if applicable\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f64d81a-1b31-4878-9a45-b64fc299452c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geo-locations-12.01.2023\n",
      "geo-locations-10.01.2023\n",
      "geo-locations-09.01.2023\n",
      "geo-locations-11.01.2023\n"
     ]
    }
   ],
   "source": [
    "indices = es.indices.get_alias(index=\"geo-locations-*\")\n",
    "for index in indices:\n",
    "    print(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a051bd44-a0e9-42c9-b753-ab0958e5b2f2",
   "metadata": {},
   "source": [
    "# Get data for specific gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab162d98-ceb3-4759-bcbf-e2ef93d9b9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_square_coordinates(center_lat, center_lon, side_length):\n",
    "    # Earth radius in meters\n",
    "    earth_radius = 6371000\n",
    "    \n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    center_lat_rad = math.radians(center_lat)\n",
    "    center_lon_rad = math.radians(center_lon)\n",
    "    \n",
    "    # Calculate the half side length of the square in meters\n",
    "    half_side_length = side_length / 2\n",
    "    \n",
    "    # Calculate the change in latitude and longitude\n",
    "    delta_lat = (half_side_length / earth_radius) * (180 / math.pi)\n",
    "    delta_lon = (half_side_length / earth_radius) * (180 / math.pi) / math.cos(center_lat_rad)\n",
    "    \n",
    "    # Calculate the top left and bottom right coordinates\n",
    "    top_left_x = center_lon - delta_lon\n",
    "    top_left_y = center_lat + delta_lat\n",
    "    bottom_right_x = center_lon + delta_lon\n",
    "    bottom_right_y = center_lat - delta_lat\n",
    "    \n",
    "    return top_left_x, top_left_y, bottom_right_x, bottom_right_y\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "0c243ce5-2be0-4084-a216-0d9dbfc50bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "center_latitude = 46.05\n",
    "center_longitude = 14.51\n",
    "top_left_x, top_left_y, bottom_right_x, bottom_right_y = calculate_square_coordinates(center_latitude, center_longitude,500)\n",
    "gym_number = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b0c7a632-2621-443d-b2d0-140f29f32252",
   "metadata": {},
   "outputs": [],
   "source": [
    "center_latitude = 46.061345\n",
    "center_longitude = 14.541045\n",
    "top_left_x, top_left_y, bottom_right_x, bottom_right_y = calculate_square_coordinates(center_latitude, center_longitude,500)\n",
    "gym_number = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "af06deab-a7e3-46dc-9544-2c1bd0828727",
   "metadata": {},
   "outputs": [],
   "source": [
    "center_latitude = 46.059817\n",
    "center_longitude = 14.518447\n",
    "top_left_x, top_left_y, bottom_right_x, bottom_right_y = calculate_square_coordinates(center_latitude, center_longitude,500)\n",
    "gym_number = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "beb60d46-4ebf-4150-b170-30990ebac2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "center_latitude = 46.245762\n",
    "center_longitude = 15.278584\n",
    "top_left_x, top_left_y, bottom_right_x, bottom_right_y = calculate_square_coordinates(center_latitude, center_longitude,500)\n",
    "gym_number = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "bec271f3-b2a2-4947-bcc2-7ad66ace778e",
   "metadata": {},
   "outputs": [],
   "source": [
    "center_latitude = 46.076444 \n",
    "center_longitude = 14.510828\n",
    "top_left_x, top_left_y, bottom_right_x, bottom_right_y = calculate_square_coordinates(center_latitude, center_longitude,500)\n",
    "gym_number = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "155c2bbb-3828-4402-910f-40b6c77d3aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "center_latitude = 46.059904\n",
    "center_longitude = 14.534631\n",
    "top_left_x, top_left_y, bottom_right_x, bottom_right_y = calculate_square_coordinates(center_latitude, center_longitude,500)\n",
    "gym_number = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "0e0e07fb-92d7-4004-81e9-8033c793150d",
   "metadata": {},
   "outputs": [],
   "source": [
    "center_latitude = 46.069197 \n",
    "center_longitude = 14.543612\n",
    "top_left_x, top_left_y, bottom_right_x, bottom_right_y = calculate_square_coordinates(center_latitude, center_longitude,500)\n",
    "gym_number = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05e24f0-8c07-4597-8ea7-1b372c0e7299",
   "metadata": {},
   "source": [
    "## DAY 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "bfc8c976-7e90-4e5c-9572-910ec61fe899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define your search query\n",
    "search_query = {\n",
    "  \"query\": {\n",
    "    \"bool\": {\n",
    "      \"must\": {\n",
    "        \"match_all\": {}\n",
    "      },\n",
    "      \"filter\": {\n",
    "        \"geo_bounding_box\": {\n",
    "          \"location\": {\n",
    "            \"top_left\": [top_left_x, top_left_y],\n",
    "            \"bottom_right\": [bottom_rigth_x, bottom_rigth_y]\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"size\": 10000\n",
    "}\n",
    "# Execute the search query\n",
    "response = es.search(index=\"geo-locations-12.01.2023\",\n",
    "                     body=search_query,\n",
    "                     scroll='1m'  # Keep the scroll window open for 1 minute\n",
    "                     )\n",
    "\n",
    "# Initialize an empty list to store the search results\n",
    "search_results = []\n",
    "\n",
    "# Get the scroll ID\n",
    "scroll_id = response['_scroll_id']\n",
    "\n",
    "# Keep scrolling while we have data\n",
    "while len(response['hits']['hits']):\n",
    "    # Process hits here\n",
    "    print(\"Received:\", len(response['hits']['hits']), \"hits\")\n",
    "    # Add hits to the search_results list\n",
    "    search_results.extend(response['hits']['hits'])\n",
    "\n",
    "    # Check if we have reached the maximum limit of 10,000 instances\n",
    "    if len(search_results) >= 300000:\n",
    "        break\n",
    "\n",
    "    # Make a request using the Scroll API\n",
    "    response = es.scroll(\n",
    "        scroll_id=scroll_id,\n",
    "        scroll='1m'  # Extend the scroll window for another minute\n",
    "    )\n",
    "\n",
    "# Clear the scroll when you're done\n",
    "es.clear_scroll(scroll_id=scroll_id)\n",
    "\n",
    "# Define the filename to store the search results\n",
    "day_date = \"12.01.2023\"\n",
    "output_file = f\"gym_{gym_number}_{day_date}.json\"\n",
    "\n",
    "# Write the search results to the output file\n",
    "with open(output_file, \"w\") as f:\n",
    "    # Write the search_results list to the JSON file\n",
    "    json.dump(search_results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "f3982bdd-48a5-464c-88f0-696ca3054c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define your search query\n",
    "search_query = {\n",
    "  \"query\": {\n",
    "    \"bool\": {\n",
    "      \"must\": {\n",
    "        \"match_all\": {}\n",
    "      },\n",
    "      \"filter\": {\n",
    "        \"geo_bounding_box\": {\n",
    "          \"location\": {\n",
    "            \"top_left\": [top_left_x, top_left_y],\n",
    "            \"bottom_right\": [bottom_rigth_x, bottom_rigth_y]\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"size\": 10000\n",
    "}\n",
    "# Execute the search query\n",
    "response = es.search(index=\"geo-locations-10.01.2023\",\n",
    "                     body=search_query,\n",
    "                     scroll='1m'  # Keep the scroll window open for 1 minute\n",
    "                     )\n",
    "\n",
    "# Initialize an empty list to store the search results\n",
    "search_results = []\n",
    "\n",
    "# Get the scroll ID\n",
    "scroll_id = response['_scroll_id']\n",
    "\n",
    "# Keep scrolling while we have data\n",
    "while len(response['hits']['hits']):\n",
    "    # Process hits here\n",
    "    print(\"Received:\", len(response['hits']['hits']), \"hits\")\n",
    "    # Add hits to the search_results list\n",
    "    search_results.extend(response['hits']['hits'])\n",
    "\n",
    "    # Check if we have reached the maximum limit of 10,000 instances\n",
    "    if len(search_results) >= 300000:\n",
    "        break\n",
    "\n",
    "    # Make a request using the Scroll API\n",
    "    response = es.scroll(\n",
    "        scroll_id=scroll_id,\n",
    "        scroll='1m'  # Extend the scroll window for another minute\n",
    "    )\n",
    "\n",
    "# Clear the scroll when you're done\n",
    "es.clear_scroll(scroll_id=scroll_id)\n",
    "\n",
    "# Define the filename to store the search results\n",
    "day_date = \"10.01.2023\"\n",
    "output_file = f\"gym_{gym_number}_{day_date}.json\"\n",
    "\n",
    "# Write the search results to the output file\n",
    "with open(output_file, \"w\") as f:\n",
    "    # Write the search_results list to the JSON file\n",
    "    json.dump(search_results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "c37a5c21-cda4-4c5a-a394-d5ffbb3f811c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define your search query\n",
    "search_query = {\n",
    "  \"query\": {\n",
    "    \"bool\": {\n",
    "      \"must\": {\n",
    "        \"match_all\": {}\n",
    "      },\n",
    "      \"filter\": {\n",
    "        \"geo_bounding_box\": {\n",
    "          \"location\": {\n",
    "            \"top_left\": [top_left_x, top_left_y],\n",
    "            \"bottom_right\": [bottom_rigth_x, bottom_rigth_y]\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"size\": 10000\n",
    "}\n",
    "# Execute the search query\n",
    "response = es.search(index=\"geo-locations-09.01.2023\",\n",
    "                     body=search_query,\n",
    "                     scroll='1m'  # Keep the scroll window open for 1 minute\n",
    "                     )\n",
    "\n",
    "# Initialize an empty list to store the search results\n",
    "search_results = []\n",
    "\n",
    "# Get the scroll ID\n",
    "scroll_id = response['_scroll_id']\n",
    "\n",
    "# Keep scrolling while we have data\n",
    "while len(response['hits']['hits']):\n",
    "    # Process hits here\n",
    "    print(\"Received:\", len(response['hits']['hits']), \"hits\")\n",
    "    # Add hits to the search_results list\n",
    "    search_results.extend(response['hits']['hits'])\n",
    "\n",
    "    # Check if we have reached the maximum limit of 10,000 instances\n",
    "    if len(search_results) >= 300000:\n",
    "        break\n",
    "\n",
    "    # Make a request using the Scroll API\n",
    "    response = es.scroll(\n",
    "        scroll_id=scroll_id,\n",
    "        scroll='1m'  # Extend the scroll window for another minute\n",
    "    )\n",
    "\n",
    "# Clear the scroll when you're done\n",
    "es.clear_scroll(scroll_id=scroll_id)\n",
    "\n",
    "# Define the filename to store the search results\n",
    "day_date = \"09.01.2023\"\n",
    "output_file = f\"gym_{gym_number}_{day_date}.json\"\n",
    "\n",
    "# Write the search results to the output file\n",
    "with open(output_file, \"w\") as f:\n",
    "    # Write the search_results list to the JSON file\n",
    "    json.dump(search_results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "b3d46c16-6c45-4b25-b78c-8e170562c436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n",
      "Received: 10000 hits\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define your search query\n",
    "search_query = {\n",
    "  \"query\": {\n",
    "    \"bool\": {\n",
    "      \"must\": {\n",
    "        \"match_all\": {}\n",
    "      },\n",
    "      \"filter\": {\n",
    "        \"geo_bounding_box\": {\n",
    "          \"location\": {\n",
    "            \"top_left\": [top_left_x, top_left_y],\n",
    "            \"bottom_right\": [bottom_rigth_x, bottom_rigth_y]\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"size\": 10000\n",
    "}\n",
    "# Execute the search query\n",
    "response = es.search(index=\"geo-locations-11.01.2023\",\n",
    "                     body=search_query,\n",
    "                     scroll='1m'  # Keep the scroll window open for 1 minute\n",
    "                     )\n",
    "\n",
    "# Initialize an empty list to store the search results\n",
    "search_results = []\n",
    "\n",
    "# Get the scroll ID\n",
    "scroll_id = response['_scroll_id']\n",
    "\n",
    "# Keep scrolling while we have data\n",
    "while len(response['hits']['hits']):\n",
    "    # Process hits here\n",
    "    print(\"Received:\", len(response['hits']['hits']), \"hits\")\n",
    "    # Add hits to the search_results list\n",
    "    search_results.extend(response['hits']['hits'])\n",
    "\n",
    "    # Check if we have reached the maximum limit of 10,000 instances\n",
    "    if len(search_results) >= 300000:\n",
    "        break\n",
    "\n",
    "    # Make a request using the Scroll API\n",
    "    response = es.scroll(\n",
    "        scroll_id=scroll_id,\n",
    "        scroll='1m'  # Extend the scroll window for another minute\n",
    "    )\n",
    "\n",
    "# Clear the scroll when you're done\n",
    "es.clear_scroll(scroll_id=scroll_id)\n",
    "\n",
    "# Define the filename to store the search results\n",
    "day_date = \"11.01.2023\"\n",
    "output_file = f\"gym_{gym_number}_{day_date}.json\"\n",
    "\n",
    "# Write the search results to the output file\n",
    "with open(output_file, \"w\") as f:\n",
    "    # Write the search_results list to the JSON file\n",
    "    json.dump(search_results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "61ff4cfb-c4f3-473a-bfee-65b46c297743",
   "metadata": {},
   "outputs": [],
   "source": [
    "for day in indices:\n",
    "    # Load JSON data\n",
    "    day_date = day.split(\"-\")[-1] \n",
    "    input_file = f\"gym_{gym_number}_{day_date}.json\"\n",
    "    with open(input_file, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    \n",
    "    # Extract relevant information and track lowest and highest times for each MSISDN\n",
    "    msisdn_data = defaultdict(list)\n",
    "    for entry in data:\n",
    "        msisdn = entry['_source']['msisdn']\n",
    "        dateTimeEvent = entry['_source']['dateTimeEvent']\n",
    "        msisdn_data[msisdn].append(dateTimeEvent)\n",
    "    \n",
    "    # Prepare filtered data\n",
    "    filtered_data = []\n",
    "    for msisdn, entries in msisdn_data.items():\n",
    "        entries.sort()  # Sort entries by dateTimeEvent\n",
    "        if len(entries) >= 2:\n",
    "            time_arrived = datetime.strptime(entries[0], \"%d.%m.%Y %H:%M:%S\")\n",
    "            time_left = datetime.strptime(entries[-1], \"%d.%m.%Y %H:%M:%S\")\n",
    "            time_difference = (time_left - time_arrived).total_seconds() / 60  # in minutes\n",
    "            if time_difference > 30 and time_difference / 60 <= 4:  # Remove instances less than 30 minutes and more than 4 hours\n",
    "                filtered_data.append([entries[0], entries[-1]])\n",
    "    \n",
    "    # Write to CSV file\n",
    "    output_file = f\"gym_{gym_number}_{day_date}.csv\"\n",
    "    with open(output_file, 'w', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(['Time_arrived', 'Time_left'])\n",
    "        writer.writerows(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "d5988b81-5a16-4073-9584-fac41f455bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to round time to nearest 15-minute interval\n",
    "def round_time_to_15_minutes(dt):\n",
    "    return dt - timedelta(minutes=dt.minute % 15,\n",
    "                          seconds=dt.second,\n",
    "                          microseconds=dt.microsecond)\n",
    "\n",
    "for day in indices:\n",
    "    # Initialize dictionary to store counts for each 15-minute interval\n",
    "    interval_counts = {}\n",
    "    day_date = day.split(\"-\")[-1] \n",
    "    \n",
    "    # Define the date from the 'day' variable\n",
    "    start_time = datetime.strptime(f\"{day_date} 00:00:00\", \"%d.%m.%Y %H:%M:%S\")\n",
    "    end_time = datetime.strptime(f\"{day_date} 23:59:59\", \"%d.%m.%Y %H:%M:%S\")\n",
    "    \n",
    "    # Define 15-minute intervals throughout the day\n",
    "    current_time = start_time\n",
    "    interval_id = 1\n",
    "    while current_time <= end_time:\n",
    "        interval_counts[str(interval_id)] = 0\n",
    "        interval_id += 1\n",
    "        current_time += timedelta(minutes=15)\n",
    "    \n",
    "    # Parse the CSV file and count instances touching each interval\n",
    "    input_file = f\"gym_{gym_number}_{day_date}.csv\"\n",
    "    with open(input_file, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)  # Skip header row\n",
    "        for row in reader:\n",
    "            time_arrived = datetime.strptime(row[0], \"%d.%m.%Y %H:%M:%S\")\n",
    "            time_left = datetime.strptime(row[1], \"%d.%m.%Y %H:%M:%S\")\n",
    "    \n",
    "            # Round arrival and departure times to nearest 15-minute interval\n",
    "            rounded_arrival_time = round_time_to_15_minutes(time_arrived)\n",
    "            rounded_departure_time = round_time_to_15_minutes(time_left)\n",
    "    \n",
    "            # Increment counts for each interval touched by the instance\n",
    "            current_time = start_time\n",
    "            interval_id = 1\n",
    "            while current_time <= end_time:\n",
    "                if rounded_arrival_time <= current_time < rounded_departure_time:\n",
    "                    interval_counts[str(interval_id)] += 1\n",
    "                interval_id += 1\n",
    "                current_time += timedelta(minutes=15)\n",
    "    \n",
    "    # Write the counts to a new CSV file\n",
    "    output_file = f\"gym_{gym_number}_{day_date}_processed.csv\"\n",
    "    with open(output_file, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['val', 'target'])\n",
    "        for interval_id, count in interval_counts.items():\n",
    "            writer.writerow([interval_id, count])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "038e338b-74e6-4c25-b056-b4049573fd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty DataFrame to store combined data\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "for day in indices:\n",
    "    day_date = day.split(\"-\")[-1] \n",
    "    # Read each processed CSV file\n",
    "    df = pd.read_csv(f\"gym_{gym_number}_{day_date}_processed.csv\")\n",
    "    \n",
    "    # Concatenate the current DataFrame with the combined DataFrame\n",
    "    combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "# Write the combined DataFrame to a new CSV file\n",
    "combined_df.to_csv(f\"gym_{gym_number}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2352eb69-ece4-4060-91f5-aba7e313e704",
   "metadata": {},
   "source": [
    "# TRAIN MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "d0730618-f4c2-41e3-a775-0ccf71a81ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "data = pd.read_csv(f\"gym_{gym_number}.csv\")\n",
    "\n",
    "X = data[['val']]\n",
    "y = data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a94c1a-415e-49bd-8a01-151d25a89ff8",
   "metadata": {},
   "source": [
    "## Grid seach for best paramethers for Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "1493ebdc-ac39-42e5-9b11-6d9c11edfde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Best Parameters: {'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 500}\n",
      "Mean Squared Error (Best Model): 24979.93691554219\n",
      "R-squared Score (Best Model): 0.6183685101634196\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 500, 1000],  # Number of trees in the forest\n",
    "    'max_depth': [None, 10, 20, 30],  # Maximum number of levels in tree\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split a node\n",
    "    'min_samples_leaf': [1, 2, 4]  # Minimum number of samples required at each leaf node\n",
    "}\n",
    "\n",
    "# Create the grid search model\n",
    "grid_search = GridSearchCV(estimator=random_forest_model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters found\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Get the best model\n",
    "best_random_forest_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model's performance\n",
    "y_pred_best = best_random_forest_model.predict(X_test)\n",
    "\n",
    "# Print evaluation metrics for the best model\n",
    "print(\"Mean Squared Error (Best Model):\", mean_squared_error(y_test, y_pred_best))\n",
    "print(\"R-squared Score (Best Model):\", r2_score(y_test, y_pred_best))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "f0d14c3b-8b3e-4085-9f22-8093186c6e87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21.32963512])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = pd.DataFrame({'val': [20]})\n",
    "best_random_forest_model.predict(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d446376-52a2-4bf7-ab3d-6420d5955bd6",
   "metadata": {},
   "source": [
    "## random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "35341845-4ceb-44ec-82de-715b155917a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 24761.63342074806\n",
      "R-squared Score: 0.6217036462062564\n"
     ]
    }
   ],
   "source": [
    "random_forest_model = RandomForestRegressor(n_estimators=100,max_depth = 10, max_features = \"sqrt\", min_samples_leaf = 4, min_samples_split = 10,  random_state=42)  # You can adjust parameters as needed\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "random_forest_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "y_pred = random_forest_model.predict(X_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n",
    "print(\"R-squared Score:\", r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "296bbd00-0934-4a62-9805-3046c7c93e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.09648479])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = pd.DataFrame({'val': [20]})\n",
    "random_forest_model.predict(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cace69-d6b9-40a8-ba72-7097f8425f61",
   "metadata": {},
   "source": [
    "## Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "c1cf91da-d58a-49e1-84f8-1b4d26134565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 58801.123228956174\n",
      "R-squared Score: 0.10166465440636074\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "#  Create a Linear Regression model\n",
    "linear_regression_model = LinearRegression()\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Train the model\n",
    "linear_regression_model.fit(X_train, y_train)\n",
    "# Evaluate the model's performance\n",
    "y_pred = linear_regression_model.predict(X_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n",
    "print(\"R-squared Score:\", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "3a9c9a9c-378b-4d34-ac17-82a47a339384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([173.27177072])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = pd.DataFrame({'val': [20]})\n",
    "linear_regression_model.predict(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "b8b3f2f5-f692-4d4b-b622-a33392de9500",
   "metadata": {},
   "outputs": [],
   "source": [
    "gym_number = 7\n",
    "data = pd.read_csv(f\"gym_{gym_number}.csv\")\n",
    "X = data[['val']]\n",
    "y = data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17742cf-6be1-40a5-89b9-dbc5f26b901f",
   "metadata": {},
   "source": [
    "## Ada boost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "00219d3a-21c4-4a74-ac7f-5768e2fb9025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (AdaBoost): 18682281.412359204\n",
      "R-squared Score (AdaBoost): 0.0731565033709679\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "# Step 1: Create an AdaBoost model\n",
    "ada_boost_model = AdaBoostRegressor(n_estimators=100, random_state=42)  # You can adjust parameters as needed\n",
    "\n",
    "# Step 2: Train the model\n",
    "ada_boost_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 3: Evaluate the model's performance\n",
    "y_pred_ada = ada_boost_model.predict(X_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Mean Squared Error (AdaBoost):\", mean_squared_error(y_test, y_pred_ada))\n",
    "print(\"R-squared Score (AdaBoost):\", r2_score(y_test, y_pred_ada))\n",
    "\n",
    "# Step 4: Make predictions\n",
    "# You can make predictions for new data using: ada_boost_model.predict(new_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e65f285-e659-4bfc-a2d2-ea943f6e8a1f",
   "metadata": {},
   "source": [
    "## Decision tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "460e632c-3652-4e1c-81b2-858680a11e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (Decision Tree): 26627897.32756133\n",
      "R-squared Score (Decision Tree): -0.32103210107031677\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Create a Decision Tree model\n",
    "decision_tree_model = DecisionTreeRegressor(random_state=42)  # You can adjust parameters as needed\n",
    "\n",
    "# Train the model\n",
    "decision_tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "y_pred_dt = decision_tree_model.predict(X_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Mean Squared Error (Decision Tree):\", mean_squared_error(y_test, y_pred_dt))\n",
    "print(\"R-squared Score (Decision Tree):\", r2_score(y_test, y_pred_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682f1231-6f3f-4dd6-9d9d-0488a95c466f",
   "metadata": {},
   "source": [
    "#  Saving the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "319d999b-8195-40ff-a688-018cdcf17a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 19054252.427384563\n",
      "R-squared Score: 0.054702712391099695\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['random_forest_model_7.joblib']"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "\n",
    "random_forest_model = RandomForestRegressor(n_estimators=100,max_depth = 10, max_features = \"sqrt\", min_samples_leaf = 4, min_samples_split = 10,  random_state=42)  # You can adjust parameters as needed\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "random_forest_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "y_pred = random_forest_model.predict(X_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n",
    "print(\"R-squared Score:\", r2_score(y_test, y_pred))\n",
    "\n",
    "#Save the trained model to a file\n",
    "dump(random_forest_model, f'random_forest_model_{gym_number}.joblib')\n",
    "\n",
    "\n",
    "#---------example case ----------------------\n",
    "# Make predictions using the trained model\n",
    "# Load the saved model from file\n",
    "#random_forest_model = load('random_forest_model.joblib')\n",
    "\n",
    "#new_data = pd.DataFrame({'val': [5, 10, 15]})\n",
    "#predictions = random_forest_model.predict(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e406361e-bb28-4fbf-9317-8eaef2f8c875",
   "metadata": {},
   "source": [
    "## Gradient Boosting regresson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "0d61543c-9dd8-4611-9f8b-796332eeaa70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (Gradient Boosting): 22901437.08200324\n",
      "R-squared Score (Gradient Boosting): -0.13615931343757826\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "#Create a Gradient Boosting model\n",
    "gb_reg = GradientBoostingRegressor(random_state=42)  # You can adjust parameters as needed\n",
    "\n",
    "# Train the model\n",
    "gb_reg.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "y_pred_gb = gb_reg.predict(X_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Mean Squared Error (Gradient Boosting):\", mean_squared_error(y_test, y_pred_gb))\n",
    "print(\"R-squared Score (Gradient Boosting):\", r2_score(y_test, y_pred_gb))\n",
    "\n",
    "# Make predictions\n",
    "# You can make predictions for new data using: gb_reg.predict(new_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2df074-c6c6-4671-9c44-9fac6afe1735",
   "metadata": {},
   "source": [
    "## LASSO regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "04ec5dbd-951b-4bb7-be39-5217ab914c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (LassoLars): 20179761.634689383\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoLars\n",
    "\n",
    "# Create a LassoLars model\n",
    "lad_reg = LassoLars(random_state=42)  # You can adjust parameters as needed\n",
    "\n",
    "#  Train the model\n",
    "lad_reg.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "y_pred_lad = lad_reg.predict(X_test)\n",
    "\n",
    "# Print evaluation metrics (if applicable for LassoLars)\n",
    "# LassoLars does not directly provide an R-squared score, so you might want to use a different evaluation metric\n",
    "print(\"Mean Squared Error (LassoLars):\", mean_squared_error(y_test, y_pred_lad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bb4548-3c76-4148-bb82-8c5e460a8218",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
